#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŒèŠ±é¡ºè‚¡ç¥¨ä¿¡æ¯çˆ¬è™« - è·å–è‚¡ç¥¨ä¸»è¥ä¸šåŠ¡ä¿¡æ¯
"""

import requests
from bs4 import BeautifulSoup
import time
import random
import logging
import pandas as pd
from urllib.parse import urljoin
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, WebDriverException
import os
from datetime import datetime

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('stock_scraper.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class StockScraper:
    def __init__(self, request_delay=2):
        """åˆå§‹åŒ–è‚¡ç¥¨çˆ¬è™«"""
        self.base_url = "https://q.10jqka.com.cn/"
        self.stock_base_url = "https://stockpage.10jqka.com.cn/"
        self.request_delay = request_delay
        self.session = requests.Session()
        
        # éšæœºUser-Agentæ± 
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/120.0.0.0 Safari/537.36'
        ]
        
        self._update_session_headers()
        self.driver = None
        self._init_driver()
        
        # å­˜å‚¨ç»“æœ
        self.stock_data = []
    
    def _get_random_user_agent(self):
        return random.choice(self.user_agents)
    
    def _update_session_headers(self):
        self.session.headers.update({
            'User-Agent': self._get_random_user_agent(),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Referer': 'https://q.10jqka.com.cn/',
        })
    
    def _init_driver(self):
        """åˆå§‹åŒ–WebDriver"""
        try:
            chrome_options = Options()
            chrome_options.add_argument('--headless')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--incognito')
            chrome_options.add_argument(f'--user-agent={self._get_random_user_agent()}')
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            chrome_options.add_argument('--disable-blink-features=AutomationControlled')
            
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            self.driver.set_page_load_timeout(60)
            logger.info("WebDriveråˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"WebDriveråˆå§‹åŒ–å¤±è´¥: {str(e)}")
            self.driver = None
    
    def debug_page_structure(self):
        """è°ƒè¯•é¡µé¢ç»“æ„ - åˆ†æé¡µé¢HTML"""
        try:
            logger.info("ğŸ” å¼€å§‹è°ƒè¯•é¡µé¢ç»“æ„...")
            
            if not self.driver:
                logger.error("WebDriveræœªåˆå§‹åŒ–")
                return
            
            self.driver.get(self.base_url)
            time.sleep(5)  # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
            
            page_source = self.driver.page_source
            soup = BeautifulSoup(page_source, 'html.parser')
            
            # æŸ¥æ‰¾æ‰€æœ‰è¡¨æ ¼
            all_tables = soup.find_all('table')
            logger.info(f"é¡µé¢ä¸­å…±æ‰¾åˆ° {len(all_tables)} ä¸ªè¡¨æ ¼")
            
            for i, table in enumerate(all_tables):
                table_classes = table.get('class', [])
                logger.info(f"è¡¨æ ¼ {i+1}: classes = {table_classes}")
                
                if 'm-table' in table_classes:
                    tbody = table.find('tbody')
                    if tbody:
                        rows = tbody.find_all('tr')
                        logger.info(f"  - æ‰¾åˆ°tbodyï¼ŒåŒ…å« {len(rows)} è¡Œ")
                        if rows:
                            first_row = rows[0]
                            cells = first_row.find_all('td')
                            logger.info(f"  - ç¬¬ä¸€è¡ŒåŒ…å« {len(cells)} åˆ—")
                            if len(cells) >= 3:
                                logger.info(f"  - ç¬¬äºŒåˆ—å†…å®¹: {cells[1].get_text(strip=True)}")
                                logger.info(f"  - ç¬¬ä¸‰åˆ—å†…å®¹: {cells[2].get_text(strip=True)}")
                    else:
                        logger.info(f"  - æœªæ‰¾åˆ°tbody")
            
            # æŸ¥æ‰¾maincont
            main_div = soup.find('div', id='maincont')
            if main_div:
                logger.info("æ‰¾åˆ°maincont div")
                tables_in_main = main_div.find_all('table')
                logger.info(f"maincontä¸­æœ‰ {len(tables_in_main)} ä¸ªè¡¨æ ¼")
            else:
                logger.info("æœªæ‰¾åˆ°maincont div")
                
        except Exception as e:
            logger.error(f"è°ƒè¯•é¡µé¢ç»“æ„å¤±è´¥: {str(e)}")

    def get_stock_list_alternative(self):
        """å¤‡ç”¨çš„è·å–è‚¡ç¥¨åˆ—è¡¨æ–¹æ³• - ä½¿ç”¨æ›´çµæ´»çš„é€‰æ‹©å™¨"""
        try:
            logger.info("ğŸ” ä½¿ç”¨å¤‡ç”¨æ–¹æ³•è·å–è‚¡ç¥¨åˆ—è¡¨...")
            
            if not self.driver:
                logger.error("WebDriveræœªåˆå§‹åŒ–")
                return []
            
            self.driver.get(self.base_url)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            wait = WebDriverWait(self.driver, 30)
            
            # å°è¯•ç­‰å¾…ä»»ä½•åŒ…å«è‚¡ç¥¨æ•°æ®çš„è¡Œ
            try:
                wait.until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "tr td a[href*='stockpage.10jqka.com.cn']"))
                )
                logger.info("âœ… æ‰¾åˆ°è‚¡ç¥¨é“¾æ¥")
            except TimeoutException:
                logger.warning("æœªæ‰¾åˆ°è‚¡ç¥¨é“¾æ¥ï¼Œç»§ç»­å°è¯•...")
            
            time.sleep(3)  # é¢å¤–ç­‰å¾…
            
            page_source = self.driver.page_source
            soup = BeautifulSoup(page_source, 'html.parser')
            
            # æŸ¥æ‰¾æ‰€æœ‰è‚¡ç¥¨é“¾æ¥
            stock_links = soup.find_all('a', href=lambda x: x and 'stockpage.10jqka.com.cn' in x)
            logger.info(f"æ‰¾åˆ° {len(stock_links)} ä¸ªè‚¡ç¥¨é“¾æ¥")
            
            stock_list = []
            processed_codes = set()  # é¿å…é‡å¤
            
            for link in stock_links:
                try:
                    # è·å–è‚¡ç¥¨ä»£ç 
                    href = link.get('href')
                    stock_code = href.split('/')[-2] if href.endswith('/') else href.split('/')[-1]
                    
                    # é¿å…é‡å¤å¤„ç†
                    if stock_code in processed_codes:
                        continue
                    processed_codes.add(stock_code)
                    
                    # è·å–è‚¡ç¥¨åç§°
                    stock_name = link.get_text(strip=True)
                    
                    # è·³è¿‡ç©ºåç§°æˆ–ä»£ç 
                    if not stock_name or not stock_code or len(stock_code) != 6:
                        continue
                    
                    # æŸ¥æ‰¾åŒ…å«æ­¤é“¾æ¥çš„è¡Œ
                    tr = link.find_parent('tr')
                    if tr:
                        cells = tr.find_all('td')
                        if len(cells) >= 3:
                            # æå–å…¶ä»–æ•°æ®
                            rank = len(stock_list) + 1
                            current_price = cells[3].get_text(strip=True) if len(cells) > 3 else '--'
                            change_percent = cells[4].get_text(strip=True) if len(cells) > 4 else '--'
                            turnover_rate = cells[7].get_text(strip=True) if len(cells) > 7 else '--'
                            market_value = cells[12].get_text(strip=True) if len(cells) > 12 else '--'
                            
                            stock_info = {
                                'rank': rank,
                                'code': stock_code,
                                'name': stock_name,
                                'url': href,
                                'current_price': current_price,
                                'change_percent': change_percent,
                                'turnover_rate': turnover_rate,
                                'market_value': market_value
                            }
                            
                            stock_list.append(stock_info)
                            logger.info(f"è·å–è‚¡ç¥¨ {rank}: {stock_code} - {stock_name}")
                            
                            # åªè·å–å‰20æ¡
                            if len(stock_list) >= 20:
                                break
                
                except Exception as e:
                    logger.error(f"å¤„ç†è‚¡ç¥¨é“¾æ¥å¤±è´¥: {str(e)}")
                    continue
            
            logger.info(f"âœ… å¤‡ç”¨æ–¹æ³•æˆåŠŸè·å– {len(stock_list)} åªè‚¡ç¥¨ä¿¡æ¯")
            return stock_list
            
        except Exception as e:
            logger.error(f"å¤‡ç”¨æ–¹æ³•è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {str(e)}")
            return []
    
    def get_stock_list(self):
        """è·å–è‚¡ç¥¨åˆ—è¡¨"""
        try:
            logger.info("ğŸ” å¼€å§‹è·å–è‚¡ç¥¨åˆ—è¡¨...")
            
            if not self.driver:
                logger.error("WebDriveræœªåˆå§‹åŒ–")
                return []
            
            self.driver.get(self.base_url)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            wait = WebDriverWait(self.driver, 30)
            
            # ç­‰å¾…ä¸»è¦å†…å®¹åŒºåŸŸåŠ è½½
            try:
                main_content = wait.until(
                    EC.presence_of_element_located((By.ID, "maincont"))
                )
                logger.info("âœ… ä¸»è¦å†…å®¹åŒºåŸŸå·²åŠ è½½")
            except TimeoutException:
                logger.warning("ä¸»è¦å†…å®¹åŒºåŸŸåŠ è½½è¶…æ—¶ï¼Œå°è¯•ç»§ç»­...")
            
            # ç­‰å¾…è¡¨æ ¼è¡ŒåŠ è½½ï¼ˆæ›´å…·ä½“çš„ç­‰å¾…æ¡ä»¶ï¼‰
            try:
                wait.until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "#maincont table tbody tr"))
                )
                logger.info("âœ… è¡¨æ ¼è¡Œå·²åŠ è½½")
            except TimeoutException:
                logger.warning("è¡¨æ ¼è¡ŒåŠ è½½è¶…æ—¶ï¼Œå°è¯•ç»§ç»­...")
            
            # é¢å¤–ç­‰å¾…ï¼Œç¡®ä¿åŠ¨æ€å†…å®¹å®Œå…¨åŠ è½½
            time.sleep(3)
            
            # è·å–é¡µé¢æºç 
            page_source = self.driver.page_source
            soup = BeautifulSoup(page_source, 'html.parser')
            
            # é¦–å…ˆå°è¯•åœ¨maincont divä¸­æŸ¥æ‰¾è¡¨æ ¼
            main_div = soup.find('div', id='maincont')
            if not main_div:
                logger.error("æœªæ‰¾åˆ°maincont div")
                return []
            
            # åœ¨maincontä¸­æŸ¥æ‰¾è¡¨æ ¼
            table = main_div.find('table', class_='m-table m-pager-table')
            if not table:
                logger.error("æœªæ‰¾åˆ°è‚¡ç¥¨è¡¨æ ¼")
                # è°ƒè¯•ï¼šè¾“å‡ºmaincontå†…å®¹çš„å‰500å­—ç¬¦
                logger.debug(f"maincontå†…å®¹é¢„è§ˆ: {str(main_div)[:500]}")
                return []
            
            tbody = table.find('tbody')
            if not tbody:
                logger.error("æœªæ‰¾åˆ°è¡¨æ ¼tbody")
                # è°ƒè¯•ï¼šè¾“å‡ºè¡¨æ ¼å†…å®¹
                logger.debug(f"è¡¨æ ¼å†…å®¹: {str(table)[:800]}")
                return []
            
            stock_list = []
            rows = tbody.find_all('tr')
            logger.info(f"æ‰¾åˆ° {len(rows)} è¡Œæ•°æ®")
            
            # åªè·å–å‰20æ¡è®°å½•
            for i, row in enumerate(rows[:20]):
                try:
                    cells = row.find_all('td')
                    if len(cells) >= 3:
                        logger.debug(f"å¤„ç†ç¬¬ {i+1} è¡Œï¼Œå…± {len(cells)} åˆ—")
                        
                        # è·å–è‚¡ç¥¨ä»£ç å’Œåç§°
                        code_cell = cells[1].find('a')
                        name_cell = cells[2].find('a')
                        
                        if code_cell and name_cell:
                            stock_code = code_cell.get_text(strip=True)
                            stock_name = name_cell.get_text(strip=True)
                            stock_url = code_cell.get('href')
                            
                            # è·å–å…¶ä»–è´¢åŠ¡æ•°æ®
                            current_price = cells[3].get_text(strip=True) if len(cells) > 3 else '--'
                            change_percent = cells[4].get_text(strip=True) if len(cells) > 4 else '--'
                            turnover_rate = cells[7].get_text(strip=True) if len(cells) > 7 else '--'
                            market_value = cells[12].get_text(strip=True) if len(cells) > 12 else '--'
                            
                            stock_info = {
                                'rank': i + 1,
                                'code': stock_code,
                                'name': stock_name,
                                'url': stock_url,
                                'current_price': current_price,
                                'change_percent': change_percent,
                                'turnover_rate': turnover_rate,
                                'market_value': market_value
                            }
                            
                            stock_list.append(stock_info)
                            logger.info(f"è·å–è‚¡ç¥¨ {i+1}: {stock_code} - {stock_name}")
                        else:
                            logger.warning(f"ç¬¬ {i+1} è¡Œç¼ºå°‘è‚¡ç¥¨ä»£ç æˆ–åç§°é“¾æ¥")
                    else:
                        logger.warning(f"ç¬¬ {i+1} è¡Œåˆ—æ•°ä¸è¶³: {len(cells)}")
                
                except Exception as e:
                    logger.error(f"è§£æç¬¬{i+1}è¡Œè‚¡ç¥¨ä¿¡æ¯å¤±è´¥: {str(e)}")
                    continue
            
            logger.info(f"âœ… æˆåŠŸè·å– {len(stock_list)} åªè‚¡ç¥¨ä¿¡æ¯")
            return stock_list
            
        except Exception as e:
            logger.error(f"è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {str(e)}")
            return []

    def _random_delay(self):
        """éšæœºå»¶è¿Ÿ"""
        delay = random.uniform(self.request_delay * 0.5, self.request_delay * 1.5)
        time.sleep(delay)
        self._update_session_headers()
    
    def get_stock_business_info(self, stock_code):
        """è·å–è‚¡ç¥¨ä¸»è¥ä¸šåŠ¡ä¿¡æ¯"""
        try:
            # æ„å»ºè¯¦æƒ…é¡µURL
            detail_url = f"{self.stock_base_url}{stock_code}/"
            logger.info(f"ğŸ“„ è·å–è‚¡ç¥¨è¯¦æƒ…: {stock_code} - {detail_url}")
            
            self._random_delay()
            
            # ä½¿ç”¨Seleniumè·å–è¯¦æƒ…é¡µï¼ˆå› ä¸ºå¯èƒ½æœ‰åŠ¨æ€å†…å®¹ï¼‰
            if self.driver:
                try:
                    self.driver.get(detail_url)
                    time.sleep(3)  # ç­‰å¾…é¡µé¢åŠ è½½
                    page_source = self.driver.page_source
                    soup = BeautifulSoup(page_source, 'html.parser')
                except Exception as e:
                    logger.warning(f"Seleniumè·å–å¤±è´¥ï¼Œå°è¯•requests: {str(e)}")
                    response = self.session.get(detail_url, timeout=30)
                    response.raise_for_status()
                    soup = BeautifulSoup(response.content, 'html.parser')
            else:
                response = self.session.get(detail_url, timeout=30)
                response.raise_for_status()
                soup = BeautifulSoup(response.content, 'html.parser')
            
            # æŸ¥æ‰¾å…¬å¸è¯¦æƒ…
            company_details = soup.find('dl', class_='company_details')
            business_info = {
                'region': '--',
                'concepts': '--',
                'listing_date': '--',
                'main_business_simple': '--',
                'main_business_detail': '--'
            }
            
            if company_details:
                dt_elements = company_details.find_all('dt')
                dd_elements = company_details.find_all('dd')
                
                for i, dt in enumerate(dt_elements):
                    if i < len(dd_elements):
                        dt_text = dt.get_text(strip=True)
                        dd = dd_elements[i]
                        dd_text = dd.get_text(strip=True)
                        
                        if 'æ‰€å±åœ°åŸŸ' in dt_text:
                            business_info['region'] = dd_text
                        elif 'æ¶‰åŠæ¦‚å¿µ' in dt_text:
                            # ä¼˜å…ˆä½¿ç”¨titleå±æ€§ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨text
                            business_info['concepts'] = dd.get('title', dd_text)
                        elif 'ä¸Šå¸‚æ—¥æœŸ' in dt_text:
                            business_info['listing_date'] = dd_text
                        elif 'ä¸»è¥ä¸šåŠ¡' in dt_text:
                            # æ£€æŸ¥æ˜¯å¦æœ‰ç»è¥åˆ†æé“¾æ¥
                            analysis_link = dd.find('a')
                            if analysis_link:
                                business_info['main_business_simple'] = 'æœ‰ç»è¥åˆ†æé“¾æ¥'
                            else:
                                # è·å–ä¸»è¥ä¸šåŠ¡ç®€è¿°
                                business_info['main_business_simple'] = dd.get('title', dd_text)
            
            # è·å–è¯¦ç»†ä¸»è¥ä¸šåŠ¡ä¿¡æ¯ - ä½¿ç”¨æ­£ç¡®çš„iframe URL
            # åŸURL: https://stockpage.10jqka.com.cn/301609/operate/
            # å®é™…iframe URL: https://basic.10jqka.com.cn/301609/operate.html#stockpage
            operate_url = f"https://basic.10jqka.com.cn/{stock_code}/operate.html#stockpage"
            logger.info(f"ğŸ“‹ è·å–ç»è¥åˆ†æ(iframe): {operate_url}")
            
            self._random_delay()
            
            # ä½¿ç”¨Seleniumè·å–iframeå†…å®¹
            main_business_detail = '--'
            if self.driver:
                try:
                    self.driver.get(operate_url)
                    wait = WebDriverWait(self.driver, 20)
                    
                    # ç­‰å¾…ä¸»è¥ä»‹ç»åŒºåŸŸåŠ è½½
                    try:
                        main_intro_element = wait.until(
                            EC.presence_of_element_located((By.CSS_SELECTOR, ".main_intro_list, #intro, .m_box.main_intro"))
                        )
                        logger.info("âœ… iframeé¡µé¢ä¸»è¥ä»‹ç»åŒºåŸŸå·²åŠ è½½")
                    except TimeoutException:
                        logger.warning("iframeé¡µé¢ä¸»è¥ä»‹ç»åŒºåŸŸåŠ è½½è¶…æ—¶")
                    
                    # é¢å¤–ç­‰å¾…ç¡®ä¿å†…å®¹å®Œå…¨åŠ è½½
                    time.sleep(5)
                    
                    operate_page_source = self.driver.page_source
                    operate_soup = BeautifulSoup(operate_page_source, 'html.parser')
                    
                    # è°ƒè¯•ï¼šæ£€æŸ¥é¡µé¢å†…å®¹
                    page_text = operate_page_source.lower()
                    has_main_business = 'ä¸»è¥ä¸šåŠ¡' in page_text
                    has_main_intro = 'main_intro_list' in page_text
                    logger.info(f"iframeé¡µé¢åŒ…å«'ä¸»è¥ä¸šåŠ¡': {has_main_business}, åŒ…å«'main_intro_list': {has_main_intro}")
                    
                    # æŸ¥æ‰¾ main_intro_list
                    main_intro_list = operate_soup.find('ul', class_='main_intro_list')
                    logger.info(f"iframeé¡µé¢main_intro_listæ‰¾åˆ°: {main_intro_list is not None}")
                    
                    if main_intro_list:
                        # æŸ¥æ‰¾åŒ…å«"ä¸»è¥ä¸šåŠ¡"çš„li
                        for li in main_intro_list.find_all('li'):
                            span_tag = li.find('span')
                            if span_tag and 'ä¸»è¥ä¸šåŠ¡' in span_tag.get_text():
                                main_business_p = li.find('p')
                                if main_business_p:
                                    main_business_detail = main_business_p.get_text(strip=True)
                                    logger.info(f"âœ… ä»iframeè·å–åˆ°ä¸»è¥ä¸šåŠ¡: {main_business_detail[:50]}...")
                                    break
                    
                    # å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
                    if main_business_detail == '--':
                        # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«"ä¸»è¥ä¸šåŠ¡ï¼š"çš„span
                        business_spans = operate_soup.find_all('span', string=lambda text: text and 'ä¸»è¥ä¸šåŠ¡' in text)
                        logger.info(f"iframeé¡µé¢æ‰¾åˆ°åŒ…å«'ä¸»è¥ä¸šåŠ¡'çš„spanå…ƒç´ æ•°é‡: {len(business_spans)}")
                        
                        for span in business_spans:
                            # æŸ¥æ‰¾åŒçº§çš„pæ ‡ç­¾
                            next_p = span.find_next_sibling('p')
                            if next_p:
                                main_business_detail = next_p.get_text(strip=True)
                                logger.info(f"âœ… é€šè¿‡iframe spanæŸ¥æ‰¾è·å–åˆ°ä¸»è¥ä¸šåŠ¡: {main_business_detail[:50]}...")
                                break
                    
                    # æœ€åçš„å¤‡ç”¨æ–¹æ³•ï¼šæŸ¥æ‰¾æ‰€æœ‰liå…ƒç´ 
                    if main_business_detail == '--':
                        all_lis = operate_soup.find_all('li')
                        logger.info(f"iframeé¡µé¢ä¸­å…±æ‰¾åˆ° {len(all_lis)} ä¸ªliå…ƒç´ ")
                        
                        for li in all_lis:
                            li_text = li.get_text()
                            if 'ä¸»è¥ä¸šåŠ¡' in li_text and 'ï¼š' in li_text and len(li_text) > 20:
                                # æå–ä¸»è¥ä¸šåŠ¡å†…å®¹
                                try:
                                    business_content = li_text.split('ï¼š', 1)[1].strip()
                                    if len(business_content) > 10:
                                        main_business_detail = business_content
                                        logger.info(f"âœ… é€šè¿‡iframe liå…ƒç´ è·å–åˆ°ä¸»è¥ä¸šåŠ¡: {main_business_detail[:50]}...")
                                        break
                                except:
                                    continue
                    
                    # è°ƒè¯•ä¿¡æ¯
                    if main_business_detail == '--':
                        logger.warning("iframeé¡µé¢ä»ç„¶æ— æ³•è·å–ä¸»è¥ä¸šåŠ¡ï¼Œè¾“å‡ºè°ƒè¯•ä¿¡æ¯:")
                        # è¾“å‡ºé¡µé¢ç‰‡æ®µç”¨äºè°ƒè¯•
                        if 'main_intro' in operate_page_source:
                            start_pos = operate_page_source.lower().find('main_intro')
                            snippet = operate_page_source[max(0, start_pos-100):start_pos+500]
                            logger.info(f"main_introåŒºåŸŸå†…å®¹ç‰‡æ®µ: {snippet}")
                
                except Exception as e:
                    logger.error(f"Seleniumè·å–iframeç»è¥åˆ†æé¡µé¢å¤±è´¥: {str(e)}")
                    
                    # å¤‡ç”¨æ–¹æ¡ˆï¼šç›´æ¥ç”¨requestsè®¿é—®iframe URL
                    try:
                        logger.info("å°è¯•ç”¨requestsç›´æ¥è®¿é—®iframe URL")
                        operate_response = self.session.get(operate_url, timeout=30)
                        operate_response.raise_for_status()
                        operate_soup = BeautifulSoup(operate_response.content, 'html.parser')
                        
                        main_intro_list = operate_soup.find('ul', class_='main_intro_list')
                        if main_intro_list:
                            for li in main_intro_list.find_all('li'):
                                span_tag = li.find('span')
                                if span_tag and 'ä¸»è¥ä¸šåŠ¡' in span_tag.get_text():
                                    main_business_p = li.find('p')
                                    if main_business_p:
                                        main_business_detail = main_business_p.get_text(strip=True)
                                        logger.info(f"âœ… é€šè¿‡requestså¤‡ç”¨æ–¹æ¡ˆè·å–åˆ°ä¸»è¥ä¸šåŠ¡: {main_business_detail[:50]}...")
                                        break
                    except Exception as e2:
                        logger.error(f"requestså¤‡ç”¨æ–¹æ¡ˆä¹Ÿå¤±è´¥: {str(e2)}")
            
            business_info['main_business_detail'] = main_business_detail
            
            logger.info(f"âœ… æˆåŠŸè·å– {stock_code} ä¸šåŠ¡ä¿¡æ¯")
            return business_info
            
        except Exception as e:
            logger.error(f"è·å– {stock_code} ä¸šåŠ¡ä¿¡æ¯å¤±è´¥: {str(e)}")
            return {
                'region': '--',
                'concepts': '--',
                'listing_date': '--',
                'main_business_simple': '--',
                'main_business_detail': '--'
            }
    
    def scrape_all_stocks(self):
        """çˆ¬å–æ‰€æœ‰è‚¡ç¥¨ä¿¡æ¯"""
        try:
            # é¦–å…ˆè°ƒè¯•é¡µé¢ç»“æ„
            self.debug_page_structure()
            
            # è·å–è‚¡ç¥¨åˆ—è¡¨
            stock_list = self.get_stock_list()
            
            # å¦‚æœä¸»æ–¹æ³•å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ–¹æ³•
            if not stock_list:
                logger.warning("ä¸»æ–¹æ³•è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ–¹æ³•...")
                stock_list = self.get_stock_list_alternative()
            
            if not stock_list:
                logger.error("æ‰€æœ‰æ–¹æ³•éƒ½æ— æ³•è·å–åˆ°è‚¡ç¥¨åˆ—è¡¨")
                return
            
            # è·å–æ¯åªè‚¡ç¥¨çš„è¯¦ç»†ä¿¡æ¯
            for stock in stock_list:
                try:
                    logger.info(f"å¤„ç†è‚¡ç¥¨ {stock['rank']}/{len(stock_list)}: {stock['code']} - {stock['name']}")
                    
                    # è·å–ä¸šåŠ¡ä¿¡æ¯
                    business_info = self.get_stock_business_info(stock['code'])
                    
                    # åˆå¹¶ä¿¡æ¯
                    complete_info = {**stock, **business_info}
                    self.stock_data.append(complete_info)
                    
                    logger.info(f"âœ… å®Œæˆè‚¡ç¥¨ {stock['code']} ä¿¡æ¯è·å–")
                    
                    # éšæœºå»¶è¿Ÿï¼Œé¿å…è¢«å°
                    self._random_delay()
                    
                except Exception as e:
                    logger.error(f"å¤„ç†è‚¡ç¥¨ {stock['code']} å¤±è´¥: {str(e)}")
                    # å³ä½¿å¤±è´¥ä¹Ÿè¦æ·»åŠ åŸºæœ¬ä¿¡æ¯
                    self.stock_data.append(stock)
                    continue
            
            logger.info(f"ğŸ‰ æ‰€æœ‰è‚¡ç¥¨ä¿¡æ¯è·å–å®Œæˆï¼Œå…± {len(self.stock_data)} æ¡è®°å½•")
            
        except Exception as e:
            logger.error(f"çˆ¬å–è‚¡ç¥¨ä¿¡æ¯å¤±è´¥: {str(e)}")
    
    def save_to_excel(self, filename=None):
        """ä¿å­˜æ•°æ®åˆ°Excelæ–‡ä»¶"""
        try:
            if not self.stock_data:
                logger.warning("æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
                return
            
            if filename is None:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"è‚¡ç¥¨ä¸»è¥ä¸šåŠ¡ä¿¡æ¯_{timestamp}.xlsx"
            
            # åˆ›å»ºDataFrame
            df = pd.DataFrame(self.stock_data)
            
            # é‡æ–°æ’åºåˆ—
            columns_order = [
                'rank', 'code', 'name', 'current_price', 'change_percent', 
                'turnover_rate', 'market_value', 'region', 'concepts', 
                'listing_date', 'main_business_simple', 'main_business_detail'
            ]
            
            # é‡å‘½ååˆ—
            column_names = {
                'rank': 'æ’å',
                'code': 'è‚¡ç¥¨ä»£ç ',
                'name': 'è‚¡ç¥¨åç§°',
                'current_price': 'ç°ä»·',
                'change_percent': 'æ¶¨è·Œå¹…(%)',
                'turnover_rate': 'æ¢æ‰‹ç‡(%)',
                'market_value': 'æµé€šå¸‚å€¼',
                'region': 'æ‰€å±åœ°åŸŸ',
                'concepts': 'æ¶‰åŠæ¦‚å¿µ',
                'listing_date': 'ä¸Šå¸‚æ—¥æœŸ',
                'main_business_simple': 'ä¸»è¥ä¸šåŠ¡ç®€è¿°',
                'main_business_detail': 'ä¸»è¥ä¸šåŠ¡è¯¦æƒ…'
            }
            
            # é€‰æ‹©å’Œé‡å‘½ååˆ—
            available_columns = [col for col in columns_order if col in df.columns]
            df = df[available_columns].rename(columns=column_names)
            
            # ä¿å­˜åˆ°Excel
            with pd.ExcelWriter(filename, engine='openpyxl') as writer:
                df.to_excel(writer, sheet_name='è‚¡ç¥¨ä¸»è¥ä¸šåŠ¡ä¿¡æ¯', index=False)
                
                # è°ƒæ•´åˆ—å®½
                worksheet = writer.sheets['è‚¡ç¥¨ä¸»è¥ä¸šåŠ¡ä¿¡æ¯']
                for column in worksheet.columns:
                    max_length = 0
                    column = [cell for cell in column]
                    for cell in column:
                        try:
                            if len(str(cell.value)) > max_length:
                                max_length = len(str(cell.value))
                        except:
                            pass
                    adjusted_width = min(max_length + 2, 50)
                    worksheet.column_dimensions[column[0].column_letter].width = adjusted_width
            
            logger.info(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°æ–‡ä»¶: {filename}")
            return filename
            
        except Exception as e:
            logger.error(f"ä¿å­˜Excelæ–‡ä»¶å¤±è´¥: {str(e)}")
            return None
    
    def close(self):
        """å…³é—­WebDriver"""
        if self.driver:
            self.driver.quit()
            logger.info("WebDriverå·²å…³é—­")

def main():
    """ä¸»å‡½æ•°"""
    scraper = StockScraper(request_delay=2)
    
    try:
        logger.info("ğŸš€ å¼€å§‹çˆ¬å–åŒèŠ±é¡ºè‚¡ç¥¨ä¿¡æ¯...")
        
        # çˆ¬å–æ‰€æœ‰è‚¡ç¥¨ä¿¡æ¯
        scraper.scrape_all_stocks()
        
        # ä¿å­˜åˆ°Excel
        filename = scraper.save_to_excel()
        
        if filename:
            logger.info(f"ğŸ‰ çˆ¬å–å®Œæˆï¼æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
        else:
            logger.error("âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥")
        
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {str(e)}")
    finally:
        scraper.close()

if __name__ == "__main__":
    main()