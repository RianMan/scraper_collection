#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸œæ–¹è´¢å¯Œæ¿å—èµ„é‡‘æµå‘çˆ¬è™« - åŸºäºSeleniumçš„å®Œæ•´ç‰ˆæœ¬
è·å–æ¿å—æˆäº¤é¢ã€ä¸»åŠ›å‡€é¢ã€æ•£æˆ·å‡€é¢æ•°æ®
"""

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, WebDriverException
import time
import random
import logging
import pandas as pd
from datetime import datetime
import re
import requests
import json

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('sector_scraper.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class EastMoneySectorScraper:
    def __init__(self, request_delay=2, headless=True):
        """åˆå§‹åŒ–çˆ¬è™«"""
        self.request_delay = request_delay
        self.headless = headless
        self.session = requests.Session()
        self.driver = None
        
        # User-Agentæ± 
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
        ]
        
        self._update_session_headers()
        self._init_driver()
        
        # å­˜å‚¨ç»“æœ
        self.sector_data = []
    
    def _get_random_user_agent(self):
        """è·å–éšæœºUser-Agent"""
        return random.choice(self.user_agents)
    
    def _update_session_headers(self):
        """æ›´æ–°session headers"""
        self.session.headers.update({
            'User-Agent': self._get_random_user_agent(),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Referer': 'https://data.eastmoney.com/',
        })
    
    def _init_driver(self):
        """åˆå§‹åŒ–WebDriver"""
        try:
            chrome_options = Options()
            if self.headless:
                chrome_options.add_argument('--headless')  # æ— å¤´æ¨¡å¼ï¼Œä¸æ˜¾ç¤ºæµè§ˆå™¨çª—å£
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--disable-blink-features=AutomationControlled')
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            chrome_options.add_argument(f'--user-agent={self._get_random_user_agent()}')
            
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            self.driver.set_page_load_timeout(60)
            
            logger.info("âœ… WebDriveråˆå§‹åŒ–æˆåŠŸï¼ˆæ— å¤´æ¨¡å¼ï¼‰")
            
        except Exception as e:
            logger.error(f"âŒ WebDriveråˆå§‹åŒ–å¤±è´¥: {str(e)}")
            logger.error("è¯·ç¡®ä¿å·²å®‰è£…ChromeDriverå¹¶æ·»åŠ åˆ°PATHä¸­")
            self.driver = None
    
    def _random_delay(self):
        """éšæœºå»¶è¿Ÿ"""
        delay = random.uniform(self.request_delay * 0.5, self.request_delay * 1.5)
        time.sleep(delay)
        self._update_session_headers()
    
    def _extract_jsonp_data(self, response_text):
        """ä»JSONPå“åº”ä¸­æå–JSONæ•°æ®"""
        try:
            pattern = r'[a-zA-Z_$][a-zA-Z0-9_$]*\((.*)\)'
            match = re.search(pattern, response_text)
            if match:
                json_str = match.group(1)
                return json.loads(json_str)
            return None
        except Exception as e:
            logger.error(f"è§£æJSONPæ•°æ®å¤±è´¥: {str(e)}")
            return None
    
    def get_sector_list(self):
        """è·å–æ‰€æœ‰æ¿å—åˆ—è¡¨ - ä½¿ç”¨APIæ–¹æ³•"""
        try:
            logger.info("ğŸ” å¼€å§‹è·å–æ¿å—åˆ—è¡¨...")
            sectors = []
            
            # è·å–æ‰€æœ‰é¡µçš„æ¿å—æ•°æ®
            page = 1
            while True:
                logger.info(f"è·å–ç¬¬ {page} é¡µæ¿å—æ•°æ®...")
                
                # æ„é€ è¯·æ±‚URL
                timestamp = int(time.time() * 1000)
                callback = f"jQuery{random.randint(10**20, 10**21-1)}_{timestamp}"
                
                url = f"https://push2.eastmoney.com/api/qt/clist/get"
                params = {
                    'np': '1',
                    'fltt': '1',
                    'invt': '2',
                    'cb': callback,
                    'fs': 'm:90+t:2+f:!50',
                    'fields': 'f12,f13,f14,f1,f2,f4,f3,f152,f20,f8,f104,f105,f128,f140,f141,f207,f208,f209,f136,f222',
                    'fid': 'f3',
                    'pn': str(page),
                    'pz': '20',
                    'po': '1',
                    'dect': '1',
                    'ut': 'fa5fd1943c7b386f172d6893dbfba10b',
                    'wbp2u': '|0|0|0|web',
                    '_': str(timestamp + random.randint(1, 100))
                }
                
                response = self.session.get(url, params=params, timeout=30)
                response.raise_for_status()
                
                # è§£æJSONPå“åº”
                data = self._extract_jsonp_data(response.text)
                if not data or data.get('rc') != 0:
                    logger.error(f"ç¬¬ {page} é¡µæ•°æ®è·å–å¤±è´¥")
                    break
                
                diff_data = data.get('data', {}).get('diff', [])
                if not diff_data:
                    logger.info(f"ç¬¬ {page} é¡µæ²¡æœ‰æ›´å¤šæ•°æ®ï¼Œç»“æŸè·å–")
                    break
                
                # è§£ææ¿å—ä¿¡æ¯
                for item in diff_data:
                    sector_info = {
                        'code': item.get('f12', ''),  # æ¿å—ä»£ç ï¼Œå¦‚BK1031
                        'name': item.get('f14', ''),  # æ¿å—åç§°
                        'market_type': item.get('f13', 90),  # å¸‚åœºç±»å‹
                    }
                    sectors.append(sector_info)
                    logger.debug(f"è·å–æ¿å—: {sector_info['code']} - {sector_info['name']}")
                
                page += 1
                self._random_delay()
                
                # å®‰å…¨é™åˆ¶ï¼šæœ€å¤šè·å–10é¡µ
                if page > 10:
                    logger.warning("è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶ï¼Œåœæ­¢è·å–")
                    break
                    
            logger.info(f"âœ… æˆåŠŸè·å– {len(sectors)} ä¸ªæ¿å—ä¿¡æ¯")
            return sectors
            
        except Exception as e:
            logger.error(f"è·å–æ¿å—åˆ—è¡¨å¤±è´¥: {str(e)}")
            return []
    
    def get_sector_trading_info(self, sector_code):
        """è·å–æ¿å—äº¤æ˜“ä¿¡æ¯ï¼ˆæˆäº¤é¢ç­‰ï¼‰- ä½¿ç”¨Selenium"""
        try:
            logger.info(f"ğŸ“Š è·å–æ¿å— {sector_code} äº¤æ˜“ä¿¡æ¯...")
            
            if not self.driver:
                logger.error("WebDriveræœªåˆå§‹åŒ–")
                return {'turnover': '--'}
            
            # è®¿é—®æ¿å—è¯¦æƒ…é¡µ
            detail_url = f"https://quote.eastmoney.com/bk/90.{sector_code}.html"
            self.driver.get(detail_url)
            
            # ç­‰å¾…æˆäº¤é¢æ•°æ®åŠ è½½ï¼ˆæœ€å¤šç­‰å¾…60ç§’ï¼‰
            max_wait_time = 60
            start_time = time.time()
            
            trading_info = {}
            while time.time() - start_time < max_wait_time:
                try:
                    # æŸ¥æ‰¾æˆäº¤é¢å…ƒç´ 
                    brief_info = self.driver.find_element(By.CLASS_NAME, "brief_info")
                    li_elements = brief_info.find_elements(By.TAG_NAME, "li")
                    
                    temp_info = {}
                    for li in li_elements:
                        text = li.text.strip()
                        if ":" in text:
                            parts = text.split(":", 1)
                            if len(parts) == 2:
                                key = parts[0].strip()
                                value = parts[1].strip()
                                temp_info[key] = value
                    
                    # æ£€æŸ¥æ˜¯å¦è·å–åˆ°æœ‰æ•ˆæ•°æ®ï¼ˆæˆäº¤é¢ä¸ä¸º"-"ï¼‰
                    if temp_info.get('æˆäº¤é¢', '-') != '-':
                        trading_info = temp_info
                        logger.info(f"âœ… æˆåŠŸè·å–æ¿å— {sector_code} äº¤æ˜“ä¿¡æ¯")
                        break
                    else:
                        time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
                        
                except Exception:
                    time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
            
            if not trading_info:
                logger.warning(f"âš ï¸  æ¿å— {sector_code} äº¤æ˜“ä¿¡æ¯è·å–è¶…æ—¶")
                trading_info = {'æˆäº¤é¢': '--'}
            
            return trading_info
            
        except Exception as e:
            logger.error(f"è·å–æ¿å— {sector_code} äº¤æ˜“ä¿¡æ¯å¤±è´¥: {str(e)}")
            return {'æˆäº¤é¢': '--'}
    
    def get_sector_fund_flow_api(self, sector_code):
        """è·å–æ¿å—èµ„é‡‘æµå‘æ•°æ® - ä½¿ç”¨APIæ–¹æ³•"""
        try:
            logger.info(f"ğŸ’° è·å–æ¿å— {sector_code} èµ„é‡‘æµå‘...")
            
            # è·å–æ€»é¡µæ•°ï¼ˆé€šè¿‡ç¬¬ä¸€æ¬¡è¯·æ±‚ï¼‰
            timestamp = int(time.time() * 1000)
            callback = f"jQuery{random.randint(10**20, 10**21-1)}_{timestamp}"
            
            api_url = "https://push2.eastmoney.com/api/qt/clist/get"
            params = {
                'cb': callback,
                'fid': 'f62',
                'po': '1',
                'pz': '50',
                'pn': '1',
                'np': '1',
                'fltt': '2',
                'invt': '2',
                'ut': '8dec03ba335b81bf4ebdf7b29ec27d15',
                'fs': f'b:{sector_code}',
                'fields': 'f12,f14,f2,f3,f62,f184,f66,f69,f72,f75,f78,f81,f84,f87,f204,f205,f124,f1,f13',
                '_': str(timestamp + random.randint(1, 100))
            }
            
            response = self.session.get(api_url, params=params, timeout=30)
            response.raise_for_status()
            
            # è§£æç¬¬ä¸€é¡µæ•°æ®è·å–æ€»æ•°
            data = self._extract_jsonp_data(response.text)
            if not data or data.get('rc') != 0:
                logger.error(f"æ¿å— {sector_code} èµ„é‡‘æµå‘APIè°ƒç”¨å¤±è´¥")
                return {'main_inflow': 0, 'retail_flow': 0}
            
            total_count = data.get('data', {}).get('total', 0)
            page_size = 50
            total_pages = (total_count + page_size - 1) // page_size if total_count > 0 else 1
            
            logger.info(f"æ¿å— {sector_code} å…±æœ‰ {total_count} åªè‚¡ç¥¨ï¼Œ{total_pages} é¡µ")
            
            # æ±‡æ€»æ‰€æœ‰é¡µé¢çš„èµ„é‡‘æµå‘æ•°æ®
            total_main_inflow = 0  # ä¸»åŠ›å‡€æµå…¥
            total_retail_flow = 0  # æ•£æˆ·å‡€æµå‘
            
            # è·å–æ¯ä¸€é¡µçš„æ•°æ®
            for page in range(1, total_pages + 1):
                logger.debug(f"è·å–ç¬¬ {page}/{total_pages} é¡µèµ„é‡‘æµå‘æ•°æ®...")
                
                # æ„é€ APIè¯·æ±‚
                timestamp = int(time.time() * 1000)
                callback = f"jQuery{random.randint(10**20, 10**21-1)}_{timestamp}"
                
                params['cb'] = callback
                params['pn'] = str(page)
                params['_'] = str(timestamp + random.randint(1, 100))
                
                api_response = self.session.get(api_url, params=params, timeout=30)
                api_response.raise_for_status()
                
                # è§£ææ•°æ®
                api_data = self._extract_jsonp_data(api_response.text)
                if not api_data or api_data.get('rc') != 0:
                    logger.warning(f"ç¬¬ {page} é¡µèµ„é‡‘æµå‘æ•°æ®è·å–å¤±è´¥")
                    continue
                
                stocks = api_data.get('data', {}).get('diff', [])
                
                for stock in stocks:
                    try:
                        # èµ„é‡‘æµå‘å­—æ®µè¯´æ˜ï¼š
                        # f62: ä»Šæ—¥ä¸»åŠ›å‡€æµå…¥ï¼ˆå·²ç»æ˜¯è¶…å¤§å•+å¤§å•çš„æ€»å’Œï¼‰
                        # f78: ä»Šæ—¥ä¸­å•å‡€æµå…¥
                        # f84: ä»Šæ—¥å°å•å‡€æµå…¥
                        
                        main_inflow = float(stock.get('f62', 0) or 0)  # ä¸»åŠ›å‡€æµå…¥
                        medium_inflow = float(stock.get('f78', 0) or 0)  # ä¸­å•å‡€æµå…¥
                        small_inflow = float(stock.get('f84', 0) or 0)  # å°å•å‡€æµå…¥
                        
                        # ä¸»åŠ› = ä¸»åŠ›å‡€æµå…¥ï¼ˆf62ï¼‰
                        total_main_inflow += main_inflow
                        
                        # æ•£æˆ· = ä¸­å•å‡€æµå…¥ + å°å•å‡€æµå…¥
                        total_retail_flow += (medium_inflow + small_inflow)
                        
                    except (ValueError, TypeError) as e:
                        logger.debug(f"è§£æè‚¡ç¥¨èµ„é‡‘æµå‘æ•°æ®å¤±è´¥: {str(e)}")
                        continue
                
                self._random_delay()
            
            # è½¬æ¢å•ä½ï¼ˆä»å…ƒè½¬ä¸ºä¸‡å…ƒï¼‰
            total_main_inflow_wan = total_main_inflow / 10000
            total_retail_flow_wan = total_retail_flow / 10000
            
            fund_flow_info = {
                'main_inflow': total_main_inflow_wan,  # ä¸»åŠ›å‡€æµå…¥ï¼ˆä¸‡å…ƒï¼‰
                'retail_flow': total_retail_flow_wan,  # æ•£æˆ·å‡€æµå‘ï¼ˆä¸‡å…ƒï¼‰
            }
            
            logger.info(f"âœ… æ¿å— {sector_code} èµ„é‡‘æµå‘æ±‡æ€»:")
            logger.info(f"   ä¸»åŠ›å‡€æµå…¥: {total_main_inflow_wan:.2f} ä¸‡å…ƒ")
            logger.info(f"   æ•£æˆ·å‡€æµå‘: {total_retail_flow_wan:.2f} ä¸‡å…ƒ")
            
            return fund_flow_info
            
        except Exception as e:
            logger.error(f"è·å–æ¿å— {sector_code} èµ„é‡‘æµå‘å¤±è´¥: {str(e)}")
            return {'main_inflow': 0, 'retail_flow': 0}
    
    def scrape_all_sectors(self, limit=None):
        """çˆ¬å–æ‰€æœ‰æ¿å—æ•°æ®"""
        try:
            logger.info("ğŸš€ å¼€å§‹çˆ¬å–æ¿å—æ•°æ®...")
            
            # è·å–æ¿å—åˆ—è¡¨
            sectors = self.get_sector_list()
            if not sectors:
                logger.error("æ— æ³•è·å–æ¿å—åˆ—è¡¨")
                return
            
            # é™åˆ¶å¤„ç†æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
            if limit:
                sectors = sectors[:limit]
                logger.info(f"é™åˆ¶å¤„ç†å‰ {limit} ä¸ªæ¿å—")
            
            # å¤„ç†æ¯ä¸ªæ¿å—
            for i, sector in enumerate(sectors, 1):
                try:
                    sector_code = sector['code']
                    sector_name = sector['name']
                    
                    logger.info(f"å¤„ç†æ¿å— {i}/{len(sectors)}: {sector_code} - {sector_name}")
                    
                    # è·å–äº¤æ˜“ä¿¡æ¯ï¼ˆæˆäº¤é¢ï¼‰
                    trading_info = self.get_sector_trading_info(sector_code)
                    
                    # è·å–èµ„é‡‘æµå‘ä¿¡æ¯
                    fund_flow_info = self.get_sector_fund_flow_api(sector_code)
                    
                    # åˆå¹¶æ•°æ®
                    complete_info = {
                        'sector_code': sector_code,
                        'sector_name': sector_name,
                        'turnover': trading_info.get('æˆäº¤é¢', '--'),  # æˆäº¤é¢
                        'main_inflow': fund_flow_info['main_inflow'],  # ä¸»åŠ›å‡€é¢ï¼ˆä¸‡å…ƒï¼‰
                        'retail_flow': fund_flow_info['retail_flow'],  # æ•£æˆ·å‡€é¢ï¼ˆä¸‡å…ƒï¼‰
                        'today_open': trading_info.get('ä»Šå¼€', '--'),
                        'today_high': trading_info.get('æœ€é«˜', '--'),
                        'today_low': trading_info.get('æœ€ä½', '--'),
                        'yesterday_close': trading_info.get('æ˜¨æ”¶', '--'),
                        'volume': trading_info.get('æˆäº¤é‡', '--'),
                        'market_value': trading_info.get('æµé€šå¸‚å€¼', '--'),
                    }
                    
                    self.sector_data.append(complete_info)
                    
                    logger.info(f"âœ… å®Œæˆæ¿å— {sector_code} - {sector_name}")
                    logger.info(f"   æˆäº¤é¢: {trading_info.get('æˆäº¤é¢', '--')}")
                    logger.info(f"   ä¸»åŠ›å‡€æµå…¥: {fund_flow_info['main_inflow']:.2f} ä¸‡å…ƒ")
                    logger.info(f"   æ•£æˆ·å‡€æµå‘: {fund_flow_info['retail_flow']:.2f} ä¸‡å…ƒ")
                    
                    self._random_delay()
                    
                except Exception as e:
                    logger.error(f"å¤„ç†æ¿å— {sector.get('code', '')} å¤±è´¥: {str(e)}")
                    continue
            
            logger.info(f"ğŸ‰ æ‰€æœ‰æ¿å—æ•°æ®è·å–å®Œæˆï¼Œå…± {len(self.sector_data)} æ¡è®°å½•")
            
        except Exception as e:
            logger.error(f"çˆ¬å–æ¿å—æ•°æ®å¤±è´¥: {str(e)}")
    
    def save_to_excel(self, filename=None):
        """ä¿å­˜æ•°æ®åˆ°Excelæ–‡ä»¶"""
        try:
            if not self.sector_data:
                logger.warning("æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
                return None
            
            if filename is None:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"æ¿å—èµ„é‡‘æµå‘æ•°æ®_{timestamp}.xlsx"
            
            # åˆ›å»ºDataFrame
            df = pd.DataFrame(self.sector_data)
            
            # é‡å‘½ååˆ—
            column_names = {
                'sector_code': 'æ¿å—ä»£ç ',
                'sector_name': 'æ¿å—åç§°',
                'turnover': 'æˆäº¤é¢',
                'main_inflow': 'ä¸»åŠ›å‡€é¢(ä¸‡å…ƒ)',
                'retail_flow': 'æ•£æˆ·å‡€é¢(ä¸‡å…ƒ)',
                'today_open': 'ä»Šå¼€',
                'today_high': 'æœ€é«˜',
                'today_low': 'æœ€ä½',
                'yesterday_close': 'æ˜¨æ”¶',
                'volume': 'æˆäº¤é‡',
                'market_value': 'æµé€šå¸‚å€¼',
            }
            
            df = df.rename(columns=column_names)
            
            # ä¿å­˜åˆ°Excel
            with pd.ExcelWriter(filename, engine='openpyxl') as writer:
                df.to_excel(writer, sheet_name='æ¿å—èµ„é‡‘æµå‘', index=False)
                
                # è°ƒæ•´åˆ—å®½
                worksheet = writer.sheets['æ¿å—èµ„é‡‘æµå‘']
                for column in worksheet.columns:
                    max_length = 0
                    column = [cell for cell in column]
                    for cell in column:
                        try:
                            if len(str(cell.value)) > max_length:
                                max_length = len(str(cell.value))
                        except:
                            pass
                    adjusted_width = min(max_length + 2, 50)
                    worksheet.column_dimensions[column[0].column_letter].width = adjusted_width
            
            logger.info(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°æ–‡ä»¶: {filename}")
            return filename
            
        except Exception as e:
            logger.error(f"ä¿å­˜Excelæ–‡ä»¶å¤±è´¥: {str(e)}")
            return None
    
    def save_summary_csv(self, filename=None):
        """ä¿å­˜æ ¸å¿ƒæ•°æ®åˆ°CSVï¼ˆæ¿å—ã€æˆäº¤é¢ã€ä¸»åŠ›å‡€é¢ã€æ•£æˆ·å‡€é¢ï¼‰"""
        try:
            if not self.sector_data:
                logger.warning("æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
                return None
            
            if filename is None:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"æ¿å—èµ„é‡‘æµå‘æ ¸å¿ƒæ•°æ®_{timestamp}.csv"
            
            # åˆ›å»ºæ ¸å¿ƒæ•°æ®DataFrame
            summary_data = []
            for item in self.sector_data:
                summary_data.append({
                    'æ¿å—': item['sector_name'],
                    'æˆäº¤é¢': item['turnover'],
                    'ä¸»åŠ›å‡€é¢': f"{item['main_inflow']:.2f}ä¸‡å…ƒ",
                    'æ•£æˆ·å‡€é¢': f"{item['retail_flow']:.2f}ä¸‡å…ƒ"
                })
            
            df = pd.DataFrame(summary_data)
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            
            logger.info(f"âœ… æ ¸å¿ƒæ•°æ®å·²ä¿å­˜åˆ°CSVæ–‡ä»¶: {filename}")
            return filename
            
        except Exception as e:
            logger.error(f"ä¿å­˜CSVæ–‡ä»¶å¤±è´¥: {str(e)}")
            return None
    
    def close(self):
        """å…³é—­WebDriver"""
        if self.driver:
            self.driver.quit()
            logger.info("ğŸ”’ WebDriverå·²å…³é—­")

def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®headless=Trueä¸ºæ— å¤´æ¨¡å¼ï¼ˆä¸æ˜¾ç¤ºæµè§ˆå™¨çª—å£ï¼‰
    # è®¾ç½®headless=Falseå¯ä»¥çœ‹åˆ°æµè§ˆå™¨æ“ä½œè¿‡ç¨‹ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    scraper = EastMoneySectorScraper(request_delay=1.5, headless=True)
    
    try:
        logger.info("ğŸš€ å¼€å§‹çˆ¬å–ä¸œæ–¹è´¢å¯Œæ¿å—èµ„é‡‘æµå‘æ•°æ®...")
        
        # çˆ¬å–æ‰€æœ‰æ¿å—æ•°æ®
        # æµ‹è¯•æ—¶å¯ä»¥è®¾ç½®limit=3é™åˆ¶æ•°é‡ï¼Œæ­£å¼è¿è¡Œæ—¶å»æ‰limitå‚æ•°
        scraper.scrape_all_sectors(limit=5)  # å…ˆæµ‹è¯•5ä¸ªæ¿å—
        
        # ä¿å­˜åˆ°Excel
        excel_filename = scraper.save_to_excel()
        
        # ä¿å­˜æ ¸å¿ƒæ•°æ®åˆ°CSV
        csv_filename = scraper.save_summary_csv()
        
        if excel_filename:
            logger.info(f"ğŸ‰ çˆ¬å–å®Œæˆï¼å®Œæ•´æ•°æ®å·²ä¿å­˜åˆ°: {excel_filename}")
        if csv_filename:
            logger.info(f"ğŸ‰ æ ¸å¿ƒæ•°æ®å·²ä¿å­˜åˆ°: {csv_filename}")
        
        # æ˜¾ç¤ºæ±‡æ€»ä¿¡æ¯
        if scraper.sector_data:
            logger.info(f"ğŸ“Š æ•°æ®æ±‡æ€»:")
            logger.info(f"   æˆåŠŸå¤„ç†æ¿å—æ•°é‡: {len(scraper.sector_data)}")
            
            # æ˜¾ç¤ºå‰3ä¸ªæ¿å—çš„æ ¸å¿ƒæ•°æ®
            logger.info(f"ğŸ“‹ å‰3ä¸ªæ¿å—æ ¸å¿ƒæ•°æ®:")
            for i, item in enumerate(scraper.sector_data[:3]):
                logger.info(f"   {i+1}. {item['sector_name']}")
                logger.info(f"      æˆäº¤é¢: {item['turnover']}")
                logger.info(f"      ä¸»åŠ›å‡€é¢: {item['main_inflow']:.2f}ä¸‡å…ƒ")
                logger.info(f"      æ•£æˆ·å‡€é¢: {item['retail_flow']:.2f}ä¸‡å…ƒ")
        
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {str(e)}")
    finally:
        scraper.close()

if __name__ == "__main__":
    main()